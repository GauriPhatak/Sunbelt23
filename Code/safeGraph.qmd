---
title: "SafeGraph"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
library(reticulate)
library(tidyverse) 
library(readxl)
library(ggplot2) 
library(reshape2) 
library(grid) 
library(chron) 
library(stringr) 
library(plyr) 
library(dplyr) 
library(schoolmath) 
library(boxr) 
library(openxlsx) 
library(lubridate)
library(logr)
library(utils)
library(zoo)
library(janitor)
library(sf)
library(foreach)
library(igraph)
library(geosphere)
library(CASCORE)
library(stringr)
library(purrr)
library(tigris)
library(gifski)

```

```{python}

import pandas as pd
import numpy as np

def ReadData(path, chunksize, ct_sbst):
  n = chunksize
  file = path
  df_chunks = []
  
  for df in pd.read_csv(file, chunksize = n, iterator = True, compression='gzip'):
    df_f = df.loc[df['region'] == "OR"]
    df_chunks.append(df_f)
    master_df = pd.concat(df_chunks)
    master_df.columns

    master_df = master_df[['city','region','postal_code','date_range_start',
    'date_range_end','raw_visit_counts','raw_visitor_counts','visits_by_day',
    'visits_by_each_hour', 'poi_cbg', 'visitor_home_cbgs', 'visitor_daytime_cbgs',
    'visitor_country_of_origin', 'distance_from_home', 'median_dwell']]

  master_df = master_df[master_df['city'].isin(r.ct_sbst)]
  return master_df
  
```

## Filtering out cities that we do not require currently.

```{r, warning=FALSE}
WD <- "C:\\Users\\gauph\\Box\\Preliminary Results Coronavirus Sewer Surveillance\\Data files\\covid data files\\TestingSandbox\\RScripts"
dat <- readRDS(paste(WD, "\\..\\COVID_Data_ROutput\\ProcessedCOVIDDataInfo.rds", sep= ""))

df <- do.call(rbind.data.frame, sapply(dat, '[', 3))
df$Date <- as.Date(as.integer(df$Date), origin = "1970-01-01")

##Loading city data
hwpath = "C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/RScripts/OSDLDataExtract/data.gdb"
ct <- st_read(hwpath, layer = "City_Limits__2016_")

## cities present in ct and wastewater data
ct_sbst <- ct$city_name[ct$city_name %in% unique(df$Location)]

## Chunksize
n = 10000

## List all the files in this folder
WP <-  "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/Weekly Patterns/2020_Weekly_Patterns/"
allFiles <- list.files(WP)
OR_files <- "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/OregonWeeklyPatterns/"

```

```{r}
## Do not run this code unless the filtered data is corrupted

for (file in allFiles) {
  py_cities <- py$ReadData(paste0(WP,file),n,ct_sbst)
  write_csv(py_cities, paste0(OR_files,"WeeklyData/",str_replace(file,".gz","")))
  
  print(paste0("File completed: ",file))
 }

```

## Consolidating the 2020 data.

```{r, warning=FALSE}
## This consolidates all the 2020 data. This data is saved in total2020.csv file.
## check if the columns are same for all the data files
allFiles <- list.files(paste0(OR_files,"WeeklyData/"))
ORdat <- data.frame(matrix(ncol=4, nrow=0))
NonORdat <- data.frame(matrix(ncol = 4, nrow = 0))
dat <- data.frame(matrix(ncol = 4, nrow = 0))
censusBlock <- data.frame(matrix(ncol = 2, nrow = 0)) 

for (file in allFiles) {
  py_cities <- read.csv(paste0(OR_files,"WeeklyData/", file))
  py_cities <- py_cities %>% select(city,postal_code,date_range_start,
                                    date_range_end,raw_visit_counts,
                                    raw_visitor_counts,poi_cbg,visitor_home_cbgs,
                                    visitor_daytime_cbgs,
                                    distance_from_home,median_dwell)
  
  py_cities$date_range_start <- strftime(ymd_hms(py_cities$date_range_start),
                                         format="%Y-%m-%d")
  py_cities$date_range_end <- strftime(ymd_hms(py_cities$date_range_end),
                                       format="%Y-%m-%d")
  
  
  py_cities <- py_cities %>% 
    select(city, visitor_home_cbgs,date_range_start) %>%
    mutate(home_cbgs = str_extract_all(visitor_home_cbgs, "[0-9]+")) %>%
    unnest() %>%
    select(city, home_cbgs,date_range_start)
  
  py_cities <- as.data.frame(
    cbind(py_cities$city[seq(1,length(py_cities$city),by=2)],
          matrix(py_cities$home_cbgs,ncol = 2,byrow = TRUE),
          py_cities$date_range_start[seq(1,length(py_cities$city),by=2)]))
  
  colnames(py_cities) <- c("City","Code","Count","startDate")
  py_cities$Count <- as.numeric(py_cities$Count)
  py_cities$VistTract <- unlist(map(py_cities$Code,
                                    function(x) str_sub(x, 3,11)))
  py_cities <- py_cities %>%
    group_by(City, VistTract) %>%
    dplyr::summarize(totalcount = sum(Count, na.rm = TRUE),
                     startDate =  first(startDate),
                     code  = first(Code)) %>% 
    ungroup()
  or_visits <- py_cities[substr(py_cities$code, start = 1, stop = 2) == "41", ]
  non_or_visits <- py_cities[!(substr(py_cities$code, start = 1, stop = 2) == "41"),]
  ORdat <- rbind(ORdat, or_visits)
  NonORdat <- rbind(NonORdat, non_or_visits)
  dat <- rbind(dat, py_cities)
}

write_csv(dat, paste0(OR_files,"Total2020.csv"))
write_csv(ORdat, paste0(OR_files,"OR2020.csv"))
write_csv(NonORdat, paste0(OR_files,"NonOR2020.csv"))

```

## Filtering out non relevant columns

## Combining by city to find visitors from different CBGS visiting that particular city.

the census block code is read as

|             |                                |            |                                                         |              |
|------------|------------|------------|-------------------------|------------|
| Block Group | STATE+COUNTY+TRACT+BLOCK GROUP | 2+3+6+1=12 | Block Group 1 in Census Tract 2231 in Harris County, TX | 482012231001 |

```         
410030001002 =  

41 - oregon

003 - County

000100 - Tract

2 - block group
```

```{r}

#####  Mapping tract to city for each city in the dataset
cityCensusblock_map <- py_cities %>% 
  select(city, poi_cbg) %>% 
  group_by(city) %>%
  group_modify(~ as.data.frame(str_sub(.x$poi_cbg, 3,11))) %>%
  distinct()

colnames(cityCensusblock_map) <- c("city", "tract")

```

```{r}
## Updated code for getting the 

oregon_bg <- st_as_sf(block_groups(state = "Oregon"))
ct_sbst <- ct#[ct$city_name %in% unique(df$Location),]
oregon_bg <- st_transform(oregon_bg, crs = st_crs(ct_sbst))

countiesDat <- counties(state = "OR")
oregon_bg$countyName <- countiesDat$NAME[match(oregon_bg$COUNTYFP, countiesDat$COUNTYFP)]


bg_intersect <- st_intersection(ct_sbst,oregon_bg)
cities <- as.data.frame(cbind(bg_intersect$GEOID, 
                              bg_intersect$city_name,
                              bg_intersect$countyName))
colnames(cities) <- c("CBGS", "city_name","county_name")
write_csv(cities,paste0(OR_files,"DistinctCensusBlocks.csv"))


```

```{r}
censusBlk$CBGS <- as.character(censusBlk$CBGS)
ORdat$code <- as.character(ORdat$code)
ORdat$countyfps <- unlist(map(ORdat$code, 
                              function(x) str_sub(x, 3,5)))
m <- match(ORdat$countyfps, countiesDat$COUNTYFP)
ORdat$county <- countiesDat$NAME[m]

m <- match(ORdat$code, censusBlk$CBGS)
ORdat$visitor_city <- censusBlk$city_name[m]

m <- match(censusBlk$city_name, 
           ct$city_name[ct$city_name %in% unique(df$Location)])
censusBlk_sbst <- censusBlk[complete.cases(censusBlk[m,]),]

m <- match(ORdat$code, censusBlk_sbst$CBGS)
ORdat$visitor_city_sbst <- censusBlk_sbst$city_name[m]

write.csv(ORdat,paste0(OR_files,"OR2020.csv"))

```

```{r}

totaldat <- read.csv(paste0(OR_files,"Total2020.csv"))
ORdat <- read.csv(paste0(OR_files,"OR2020.csv"))
NonORdat <- read.csv(paste0(OR_files,"NonOR2020.csv"))
censusBlk <- read.csv(paste0(OR_files,"DistinctCensusBlocks.csv"))

```

```{r}

## Lets take a look at whats going on with the codes that return NA even before updating based on the WW water cities. 

## These are locations that are not in the cities but might be rural regions. 
naOR <- ORdat[is.na(ORdat$visitor_city),]
gbg <- naOR[naOR$code %in% as.numeric(oregon_bg$GEOID),]

## using the data at block level. We find that the number of geoids is same as from the block_groups function.
gbg <- blocks(stat = "OR")
gbg$GEOID <- as.numeric(unlist(map(gbg$GEOID20, function(x) str_sub(x, 1,12))))
length(unique(gbg$GEOID))
#gbg$GEOID %in% oregon_bg$GEOID


```

```{r}
## Building graph using the subset of the oregon dataset including just the ww cities. 

ORsbst <- ORdat[!is.na(ORdat$visitor_city_sbst), ]

ORsbst$startDate <- parse_date_time(ORsbst$startDate, orders = "ymd")
ORsbst$epiweek <- paste(epiweek(ORsbst$startDate ), 
                        as.character(year(ORsbst$startDate )), 
                        sep= "-")
ORsbstGrpd  <- ORsbst %>% 
  subset(select = c(City, epiweek, totalcount, visitor_city_sbst, startDate)) %>%
  group_by(visitor_city_sbst, City, epiweek) %>%
  dplyr::summarise(sumCnt =  sum(totalcount),
                   date = first(startDate)) %>%
  ungroup() %>%
  arrange(date)
  
## getting the latitude longitude of all the cities
ct_cen <- ct %>% 
  st_as_sf() %>%
  st_centroid() %>%
  st_cast("POINT") %>%
  st_transform(4326) %>%
  st_coordinates() %>%
  as.data.frame()

ct_cen$ct <- ct$city_name

# Creating discrete time maps 
epiweek <- unique(ORsbstGrpd$epiweek)

## creating a backup 
ORsbstGrpdWkly <- ORsbstGrpd
  
```

```{r}



## No removing the ties within cities
ORsbstGrpd <- ORsbstGrpd[ORsbstGrpd$visitor_city_sbst != ORsbstGrpd$City, ]

## find the summary for the number of counts 
summary(ORsbstGrpd$sumCnt)
barplot(ORsbstGrpd$sumCnt)
m <- round(median(ORsbstGrpd$sumCnt))

ORsbstGrpd <- ORsbstGrpd[ORsbstGrpd$sumCnt > m, ]

#Creating the big graph using all the edges and counts as edge weights

G <- graph_from_data_frame(ORsbstGrpd, directed = TRUE, vertices = ct_cen$ct )
V(G)$lat <- as.numeric(ct_cen[,1])
V(G)$lon <- as.numeric(ct_cen[,2])

i=1
for(week in epiweek) {
  
  g <- subgraph.edges(G,
                      E(G)[E(G)$epiweek == week],
                      delete.vertices = TRUE)
  g <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = list(sumCnt = "sum"))
  png(sprintf("C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/Sunbelt23/Code\\animap\\%03d.png",i) ,width=5000, height=5000, res=372)
  plot.igraph(g, margin=0, frame=F, vertex.size = 3,vertex.frame = NA, 
              vertex.color = "darkblue", main = as.character(week),
        vertex.label=NA, edge.color="cornsilk4",
        layout=cbind(V(g)$lat, V(g)$lon), 
        edge.arrow.size = 0.01, edge.width= log(E(g)$sumCnt))
  dev.off()
  i <- i+1
  
}

png_files <- list.files(paste0(getwd()[1],"/animap"), pattern = ".*png$", full.names= TRUE)

gifski(png_files , gif_file = "ORegonSF.gif", width = 800, height = 600, delay = 1)
```

```{r}
## creating the map with median value per week. 




```
