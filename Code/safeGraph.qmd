---
title: "SafeGraph"
format: html
editor: visual
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

When you click the **Render** button a document will be generated that includes both content and the output of embedded code. You can embed code like this:

```{r}
library(reticulate)
library(tidyverse) 
library(readxl)
library(ggplot2) 
library(reshape2) 
library(grid) 
library(chron) 
library(stringr) 
library(plyr) 
library(dplyr) 
library(schoolmath) 
library(boxr) 
library(openxlsx) 
library(lubridate)
library(logr)
library(utils)
library(zoo)
library(janitor)
library(sf)
library(foreach)
library(igraph)
library(geosphere)
library(CASCORE)
library(stringr)
library(purrr)
library(tigris)
library(gifski)

```

```{python}

import pandas as pd
import numpy as np

def ReadData(path, chunksize, ct_sbst):
  n = chunksize
  file = path
  df_chunks = []
  
  for df in pd.read_csv(file, chunksize = n, iterator = True, compression='gzip'):
    df_f = df.loc[df['region'] == "OR"]
    df_chunks.append(df_f)
    master_df = pd.concat(df_chunks)
    master_df.columns

    master_df = master_df[['city','region','postal_code','date_range_start',
    'date_range_end','raw_visit_counts','raw_visitor_counts','visits_by_day',
    'visits_by_each_hour', 'poi_cbg', 'visitor_home_cbgs', 'visitor_daytime_cbgs',
    'visitor_country_of_origin', 'distance_from_home', 'median_dwell']]

  master_df = master_df[master_df['city'].isin(r.ct_sbst)]
  return master_df
  
```

## Filtering out cities that we do not require currently.

```{r, warning=FALSE}
WD <- "C:\\Users\\gauph\\Box\\Preliminary Results Coronavirus Sewer Surveillance\\Data files\\covid data files\\COVID_Data_ROutput"
dat <- readRDS(paste(WD, "\\ProcessedCOVIDDataInfo.rds", sep= ""))

df <- do.call(rbind.data.frame, sapply(dat, '[', 3))
df$Date <- as.Date(as.integer(df$Date), origin = "1970-01-01")

##Loading city data
hwpath = "C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/RScripts/OSDLDataExtract/data.gdb"
ct <- st_read(hwpath, layer = "City_Limits__2016_")

## cities present in ct and wastewater data
ct_sbst <- ct$city_name[ct$city_name %in% unique(df$Location)]

OR_files <- "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/OregonWeeklyPatterns/"
ORdat <- read.csv(paste0(OR_files,"OR2020.csv"))
## Building graph using the subset of the oregon dataset including just the ww cities. 

## Chunksize
n = 10000

## List all the files in this folder
WP <-  "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/Weekly Patterns/2020_Weekly_Patterns/"
allFiles <- list.files(WP)

```

```{r}
## Do not run this code unless the filtered data is corrupted

for (file in allFiles) {
  py_cities <- py$ReadData(paste0(WP,file),n,ct_sbst)
  write_csv(py_cities, paste0(OR_files,"WeeklyDataT/",
                              str_replace(file,".gz","")))
  print(paste0("File completed: ",file))
 }

```

```{r}
## Reading the data from backfill data
## List all the files in this folder
WP <-  "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/backfill/2020_Weekly_Patterns/ByWeek/"
allFiles <- list.files(WP, recursive = TRUE)

OR_files <- "//depot.engr.oregonstate.edu/mime_u1/agor/Safe Graph Data/OregonWeeklyPatterns/"

for (file in allFiles) {
  py_cities <- py$ReadData(paste0(WP,file),n,ct_sbst)
  if(!dir.exists(paste0(OR_files,"backfill/",str_split(file, "/")[[1]][[1]]))){
    dir.create(paste0(OR_files,"backfill/",str_split(file, "/")[[1]][[1]]))
  }
  write_csv(py_cities, paste0(OR_files,"backfill/",
                              str_replace(file,".gz","")))
  print(paste0("File completed: ",file))
}


```

## Consolidating the 2020 data.

```{r, warning=FALSE}
## This consolidates all the 2020 data. This data is saved in total2020.csv file.
## check if the columns are same for all the data files
#allFiles <- list.files(paste0(OR_files,"WeeklyData/"))
ORdat <- data.frame(matrix(ncol=4, nrow=0))
NonORdat <- data.frame(matrix(ncol = 4, nrow = 0))
dat <- data.frame(matrix(ncol = 4, nrow = 0))
censusBlock <- data.frame(matrix(ncol = 2, nrow = 0)) 

for (file in allFiles) {
  py_cities <- read.csv(paste0(OR_files,"backfill/", str_replace(file,".gz","")))
  py_cities <- py_cities %>% select(city,postal_code,date_range_start,
                                    date_range_end,raw_visit_counts,
                                    raw_visitor_counts,poi_cbg,visitor_home_cbgs,
                                    visitor_daytime_cbgs,
                                    distance_from_home,median_dwell)
  
  py_cities$date_range_start <- strftime(ymd_hms(py_cities$date_range_start),
                                         format="%Y-%m-%d")
  py_cities$date_range_end <- strftime(ymd_hms(py_cities$date_range_end),
                                       format="%Y-%m-%d")
  
  
  py_cities <- py_cities %>% 
    select(city, visitor_home_cbgs,date_range_start) %>%
    mutate(home_cbgs = str_extract_all(visitor_home_cbgs, "[0-9]+")) %>%
    unnest() %>%
    select(city, home_cbgs,date_range_start)
  
  py_cities <- as.data.frame(
    cbind(py_cities$city[seq(1,length(py_cities$city),by=2)],
          matrix(py_cities$home_cbgs,ncol = 2,byrow = TRUE),
          py_cities$date_range_start[seq(1,length(py_cities$city),by=2)]))
  
  colnames(py_cities) <- c("City","Code","Count","startDate")
  py_cities$Count <- as.numeric(py_cities$Count)
  py_cities$VistTract <- unlist(map(py_cities$Code,
                                    function(x) str_sub(x, 3,11)))
  py_cities <- py_cities %>%
    group_by(City, VistTract) %>%
    dplyr::summarize(totalcount = sum(Count, na.rm = TRUE),
                     startDate =  first(startDate),
                     code  = first(Code)) %>% 
    ungroup()
  or_visits <- py_cities[substr(py_cities$code, start = 1, stop = 2) == "41", ]
  non_or_visits <- py_cities[!(substr(py_cities$code, start = 1, stop = 2) == "41"),]
  ORdat <- rbind(ORdat, or_visits)
  NonORdat <- rbind(NonORdat, non_or_visits)
  dat <- rbind(dat, py_cities)
}

#write_csv(dat, paste0(OR_files,"Total2020.csv"))
#write_csv(ORdat, paste0(OR_files,"OR2020.csv"))
#write_csv(NonORdat, paste0(OR_files,"NonOR2020.csv"))

```

## Filtering out non relevant columns

## Combining by city to find visitors from different CBGS visiting that particular city.

the census block code is read as

|             |                                |            |                                                         |              |
|-------------|--------------------------------|------------|---------------------------------------------------------|--------------|
| Block Group | STATE+COUNTY+TRACT+BLOCK GROUP | 2+3+6+1=12 | Block Group 1 in Census Tract 2231 in Harris County, TX | 482012231001 |

```         
410030001002 =  

41 - oregon

003 - County

000100 - Tract

2 - block group
```

```{r}

#####  Mapping tract to city for each city in the dataset
cityCensusblock_map <- py_cities %>% 
  select(city, poi_cbg) %>% 
  group_by(city) %>%
  group_modify(~ as.data.frame(str_sub(.x$poi_cbg, 3,11))) %>%
  distinct()

colnames(cityCensusblock_map) <- c("city", "tract")

```

```{r}
## Updated code for getting the 

oregon_bg <- st_as_sf(block_groups(state = "Oregon"))
ct_sbst <- ct#[ct$city_name %in% unique(df$Location),]
oregon_bg <- st_transform(oregon_bg, crs = st_crs(ct_sbst))

countiesDat <- counties(state = "OR")
oregon_bg$countyName <- countiesDat$NAME[match(oregon_bg$COUNTYFP, countiesDat$COUNTYFP)]

bg_intersect <- st_intersection(ct_sbst,oregon_bg)
cities <- as.data.frame(cbind(bg_intersect$GEOID, 
                              bg_intersect$city_name,
                              bg_intersect$countyName))
colnames(cities) <- c("CBGS", "city_name","county_name")
write_csv(cities,paste0(OR_files,"DistinctCensusBlocks.csv"))


```

Creating column for visitor cities in oregon for the OR dataset

```{r}
censusBlk$CBGS <- as.character(censusBlk$CBGS)
ORdat$code <- as.character(ORdat$code)
ORdat$countyfps <- unlist(map(ORdat$code, 
                              function(x) str_sub(x, 3,5)))
m <- match(ORdat$countyfps, countiesDat$COUNTYFP)
ORdat$county <- countiesDat$NAME[m]

m <- match(ORdat$code, censusBlk$CBGS)
ORdat$visitor_city <- censusBlk$city_name[m]

m <- match(censusBlk$city_name, 
           ct$city_name[ct$city_name %in% unique(df$Location)])
censusBlk_sbst <- censusBlk[complete.cases(censusBlk[m,]),]

m <- match(ORdat$code, censusBlk_sbst$CBGS)
ORdat$visitor_city_sbst <- censusBlk_sbst$city_name[m]

write.csv(ORdat,paste0(OR_files,"OR2020.csv"))

```

creating column of visitor cities for all the visitor cities from out of oregon and in oregon

```{r}

## Currently I am using the ct hwypath spatial data for getting the city names. I need to find a dataset that gives me names of places at a both a city and other levels.

censusBlk$CBGS <- as.character(censusBlk$CBGS)
ORtotaldat$code <- as.character(ORtotaldat$code)
ORtotaldat$countyfps <- unlist(map(ORtotaldat$code, 
                              function(x) str_sub(x, 3,5)))
m <- match(ORtotaldat$countyfps, countiesDat$COUNTYFP)
ORtotaldat$county <- countiesDat$NAME[m]

m <- match(ORtotaldat$code, censusBlk$CBGS)
ORtotaldat$visitor_city <- censusBlk$city_name[m]

m <- match(censusBlk$city_name, 
            ct$city_name[ct$city_name %in% unique(df$Location)])
censusBlk_sbst <- censusBlk[complete.cases(censusBlk[m,]),]

m <- match(ORtotaldat$code, censusBlk_sbst$CBGS)
ORtotaldat$visitor_city_sbst <- censusBlk_sbst$city_name[m]

write.csv(ORtotaldat,paste0(OR_files,"OR2020.csv"))

```

```{r}

totaldat <- read.csv(paste0(OR_files,"Total2020.csv"))
ORdat <- read.csv(paste0(OR_files,"OR2020.csv"))
NonORdat <- read.csv(paste0(OR_files,"NonOR2020.csv"))
censusBlk <- read.csv(paste0(OR_files,"DistinctCensusBlocks.csv"))

```

```{r}

## Lets take a look at whats going on with the codes that return NA even before updating based on the WW water cities. 

## These are locations that are not in the cities but might be rural regions. 
naOR <- ORdat[is.na(ORdat$visitor_city),]
gbg <- naOR[naOR$code %in% as.numeric(oregon_bg$GEOID),]

## using the data at block level. We find that the number of geoids is same as from the block_groups function.
gbg <- blocks(stat = "OR")
gbg$GEOID <- as.numeric(unlist(map(gbg$GEOID20, function(x) str_sub(x, 1,12))))
length(unique(gbg$GEOID))
#gbg$GEOID %in% oregon_bg$GEOID


```

```{r}
## Building graph using the subset of the oregon dataset including just the ww cities. 

ORsbst <- ORdat[!is.na(ORdat$visitor_city_sbst), ]

ORsbst$startDate <- parse_date_time(ORsbst$startDate, orders = "ymd")
ORsbst$epiweek <- paste(epiweek(ORsbst$startDate ), 
                        as.character(year(ORsbst$startDate )), 
                        sep= "-")
ORsbstGrpd  <- ORsbst %>% 
  subset(select = c(City, epiweek, totalcount, visitor_city_sbst, startDate)) %>%
  group_by(visitor_city_sbst, City, epiweek) %>%
  dplyr::summarise(sumCnt =  sum(totalcount),
                   date = first(startDate)) %>%
  ungroup() %>%
  arrange(date)
  
## getting the latitude longitude of all the cities
ct_cen <- ct %>% 
  st_as_sf() %>%
  st_centroid() %>%
  st_cast("POINT") %>%
  st_transform(4326) %>%
  st_coordinates() %>%
  as.data.frame()

ct_cen$ct <- ct$city_name

# Creating discrete time maps 
epiweek <- unique(ORsbstGrpd$epiweek)

## creating a backup 
ORsbstGrpdWkly <- ORsbstGrpd
  
```

```{r}



## No removing the ties within cities
ORsbstGrpd <- ORsbstGrpd[ORsbstGrpd$visitor_city_sbst != ORsbstGrpd$City, ]

gbg <- ORsbstGrpd %>%
  group_by(epiweek) %>%
  dplyr::summarise(med  = median(sumCnt),
                   m = mean(sumCnt))

barplot(ORsbstGrpd$sumCnt)
m <- round(median(ORsbstGrpd$sumCnt))

ORsbstGrpd <- ORsbstGrpd %>% 
  group_by(epiweek) %>%
  filter(sumCnt > median(sumCnt)) %>%
  ungroup()
  
#ORsbstGrpd <- ORsbstGrpd[ORsbstGrpd$sumCnt > m, ]

#Creating the big graph using all the edges and counts as edge weights

G <- graph_from_data_frame(ORsbstGrpd, directed = TRUE, vertices = ct_cen$ct)
V(G)$lat <- as.numeric(ct_cen[,1])
V(G)$lon <- as.numeric(ct_cen[,2])

i=1
for(week in epiweek) {
  
  g <- subgraph.edges(G,
                      E(G)[E(G)$epiweek == week],
                      delete.vertices = TRUE)
  # g <- simplify(g, remove.multiple = TRUE, remove.loops = TRUE, edge.attr.comb = list(sumCnt = "sum"))
  png(sprintf("C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/Sunbelt23/Code\\animap\\sfgrph\\%03d.png",i),
      width=5000, height=5000, res=372)
  plot.igraph(g, margin=0, frame=F, vertex.size = 3,vertex.frame = NA, 
              vertex.color = "darkblue", main = as.character(week),
        vertex.label=NA, edge.color="cornsilk4",
        layout=cbind(V(g)$lat, V(g)$lon), 
        edge.arrow.size = 0.01)
  dev.off()
  i <- i+1
  
}

png_files <- list.files(paste0(getwd()[1],"/animap/sfgrph"), 
                        pattern = ".*png$", full.names= TRUE)

gifski(png_files , gif_file = "ORegonSFmed.gif", width = 800, height = 600, delay = 1)
```

```{r}

## Removeing vertices wihtout connections
Isolated = which(degree(G)==0)
G2 = delete_vertices(G, Isolated)

## Normalize the weekly traffic data.
E(G2)$sumCnt <- ((E(G2)$sumCnt - min(E(G2)$sumCnt))/(max(E(G2)$sumCnt) - min(E(G2)$sumCnt)))+0.01

## fitting community detection algorithm for the safegraph data for each week. 

## fitting edgebetweenness algorithm
c <- rep(0,45)
i=1
unlink("C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/Sunbelt23/Code\\animap\\edge\\*.png")
for(week in epiweek) {
  
  g <- subgraph.edges(G2,
                      E(G2)[E(G2)$epiweek == week],
                      delete.vertices = FALSE)
  mem <- cluster_edge_betweenness(g,weights = E(g)$sumCnt ,directed = FALSE)$membership
  c <- cbind(c, mem)
  png(sprintf("C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/Sunbelt23/Code\\animap\\edge\\%03d.png",i),
      width=5000, height=5000, res=372)
  plot.igraph(g, margin=0, frame=F, vertex.size = 3,vertex.frame = NA, 
              vertex.color = mem, main = as.character(week),
        vertex.label=NA, edge.color="cornsilk4",
        layout=cbind(V(g)$lat, V(g)$lon), 
        edge.arrow.size = 0.01)
  dev.off()
  i <- i+1
  
}
png_files <- list.files(paste0(getwd()[1],"/animap/edge"), 
                        pattern = ".*png$", full.names= TRUE)

gifski(png_files , gif_file = "ORegonSFedgeWwtsUndir.gif", 
       width = 800, height = 600, delay = 1)
apply(c, MARGIN = 2, function(x) length(unique(x)))

```

``` r
```

```{r}

## fitting spectral community detection algorithm
source("HelperFuncs.R")
c <- rep(0,45)
i=1
for(week in epiweek) {
  
  g <- subgraph.edges(G2,
                      E(G2)[E(G2)$epiweek == week],
                      delete.vertices = FALSE)
  mem <- RegSpectralClust(g, k = 9 )
  c <- cbind(c, mem)
  png(sprintf("C:/Users/gauph/Documents/StatisticsMS_PhD/Wastewater-Surveillance-OSU/Sunbelt23/Code\\animap\\spectral\\%03d.png",i),
      width=5000, height=5000, res=372)
  plot.igraph(g, margin=0, frame=F, vertex.size = 3,vertex.frame = NA, 
              vertex.color = mem, main = as.character(week),
        vertex.label=NA, edge.color="cornsilk4",
        layout=cbind(V(g)$lat, V(g)$lon), 
        edge.arrow.size = 0.01)
  dev.off()
  
}
png_files <- list.files(paste0(getwd()[1],"/animap/spectral"), 
                        pattern = ".*png$", full.names= TRUE)

gifski(png_files , gif_file = "ORegonSFspectral.gif", 
       width = 800, height = 600, delay = 1)


```

```{r}

source("HelperFuncs.R") 

### Reading the OR data. At this point the data does not contain within city ties and does not contain any cities other than wastewater cities.
# Set multiple thresholds for cutoffs

thresh <- c(0.10,0.20,0.30,0.40,0.50,0.60,0.70,0.80,0.90)

comlist <- list()

## create a subset of just the wastewater cities
NotwwCty <- !(ct_cen$ct %in% (ORsbstGrpd %>% 
                pivot_longer(cols = c('visitor_city_sbst', 'City')) %>% 
                select(value) %>% 
                unique())$value)

for( t in thresh){
  
  ## Create networks considering all the thresholds above. 
  TG <- ORsbstGrpd %>% 
    dplyr::group_by(epiweek) %>%
    dplyr::mutate(q = quantile(sumCnt, probs = t))%>%
    filter(sumCnt > q) %>%
     ungroup()
  
  #Creating the big graph using all the edges and counts as edge weights
  G <- graph_from_data_frame(TG, directed = TRUE, vertices = ct_cen$ct)
  V(G)$lat <- as.numeric(ct_cen[,1])
  V(G)$lon <- as.numeric(ct_cen[,2])
  ## Removing vertices wihtout connections
  #Isolated = which(degree(G)==0)
  G2 = delete_vertices(G, NotwwCty)
  ## Normalize the weekly traffic data.
  #E(G2)$sumCnt <- ((E(G2)$sumCnt - min(E(G2)$sumCnt))/(max(E(G2)$sumCnt) - min(E(G2)$sumCnt)))+0.01
  ## Applying Community detection methods to the networks formed above.
  
###### DISCUSS THE ISSUE WITH USING WEIGHTED EDGEBETWEENNESS FROM IGRAPH DIRECTLY https://github.com/igraph/igraph/issues/1040   page 24 : https://arxiv.org/pdf/0906.0612.pdf #######
  
###### Edge betweenness based community detection is works by repeatedly cutting the edge with the highest edge betweenness. This way it constructs a "dendrogram", i.e. a series of possible clusterings. Then it selects the one with the highest modularity.Betweenness calculations are based on the concept of graph distance. When the algorithm is given weights, it interprets it as the "length"/"distance" of that edge. Conceptually, vertices connected with a "short" / "low weight" edge are more tightly coupled than those connected by a "long" / "high weight" edge.But modularity calculations use weights the opposite way. It is high weight values (instead of low ones) that indicate tighter coupling.###### 
  
########## Might have to use my implementation for edgebetweenness algorithm. Unfortunately it is very slow and needs me to implement a more efficient version. ###### 
  
  c <- as.data.frame(matrix(nrow = 46, ncol=0))#rep(0,46)
  d <- as.data.frame(matrix(nrow = 46, ncol=0))#rep(0,46)
  i=1
  for(week in epiweek) {
    
    g <- subgraph.edges(G2,
                        E(G2)[E(G2)$epiweek == week],
                        delete.vertices = FALSE)
      ## 1. Applying Edge-betweenness
    mem <- cluster_edge_betweenness(g,
                                    weights = E(g)$sumCnt ,
                                    directed = TRUE, 
                                    modularity = TRUE, 
                                    membership = TRUE)$membership
    c[,i] <- mem
      ## 2. Applying Spectral CLustering algorithm
    mem <- RegSpectralClust(g, k = 9 )
    d[,i] <- mem


    i <- i+1
    
  }
  
  comlist[[as.character(t)]] <- c
  
  ## Descriptive summaries of the various generated networks.
  
  ## 1. Network structure descriptions
  ## 2. Community detection over time for various networks.
  
  ## Incorporate wastewater measurements as covariates ... For missing values reduce the number of missing values. 
  
  ## apply STERGM model for all the created networks.
  
  
}




```
